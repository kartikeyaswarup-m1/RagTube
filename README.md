# ğŸ¥ RagTube â€” YouTube Video Q&A Assistant

**RagTube** is a Retrieval-Augmented Generation (RAG) based system that lets you **ask questions about any YouTube video**.  
It fetches the video transcript, splits it into chunks, generates embeddings, stores them in FAISS, and uses a local LLM (via Ollama) to answer user queries.

---

## ğŸš€ Features

- Fetches YouTube transcripts automatically (manual or auto captions)
- Cleans and chunks transcripts into meaningful sections
- Generates embeddings locally using Ollama
- Stores vectors in FAISS (local vector database)
- Retrieves the most relevant chunks for a given question
- Answers questions using a local LLM model (offline!)

---

## ğŸ§  Tech Stack

| Component | Technology |
|------------|-------------|
| **Backend** | FastAPI (Python) |
| **LLM Runtime** | Ollama |
| **Vector Database** | FAISS |
| **Embeddings** | `nomic-embed-text` (via Ollama) |
| **Language Model** | Default: `phi3` |
| **Transcript Fetching** | yt-dlp |
| **Language** | Python 3.10+ |

------------------------------------------
Steps to run the project-

ğŸ§© 1ï¸âƒ£ Clone the project

Open PowerShell, Git Bash, or VS Code terminal, and run:

git clone https://github.com/kartikeyaswarup-m1/RagTube.git
cd RagTube/backend

ğŸ§© 2ï¸âƒ£ Create and activate a virtual environment
ğŸ”¹ On Windows:
python -m venv venv
venv\Scripts\activate


After this, your terminal should start with (venv) â€” this means itâ€™s activated.

ğŸ§© 3ï¸âƒ£ Install dependencies

Run:

pip install -r requirements.txt


If thereâ€™s no requirements.txt, use this instead:

pip install fastapi uvicorn yt-dlp requests ollama faiss-cpu numpy python-dotenv

ğŸ§© 4ï¸âƒ£ Install and set up Ollama

Download Ollama from:
ğŸ‘‰ https://ollama.com/download

After installation, open a new terminal and test:

ollama --version


Pull the models used in this project:

ollama pull phi3
ollama pull nomic-embed-text


âš ï¸ This may take a few minutes (models download once).

ğŸ§© 5ï¸âƒ£ Check the .env file

In the backend folder, thereâ€™s a file named .env.
It already has all required settings.

Make sure it looks like this:

VECTORSTORE_DIR=./vectorstore

OLLAMA_MODEL=phi3
EMBED_MODEL=nomic-embed-text
OLLAMA_HOST=http://127.0.0.1:11434

BACKEND_HOST=127.0.0.1
BACKEND_PORT=8000


ğŸ” If you want to use another model (like llama3), just change this line:

OLLAMA_MODEL=llama3


and make sure to pull it using ollama pull llama3.

ğŸ§© 6ï¸âƒ£ Run the backend

From the project root folder (RagTube):

uvicorn backend.app.main:app --reload


If everything is okay, youâ€™ll see:

INFO:     Uvicorn running on http://127.0.0.1:8000

ğŸ§© 7ï¸âƒ£ Open the API Docs

Go to your browser and open:
ğŸ‘‰ http://127.0.0.1:8000/docs

This page shows all available endpoints:

/ingest â€” to load a YouTube video transcript

/query â€” to ask questions about the video

ğŸ§© 8ï¸âƒ£ Try it out!
ğŸ”¹ Step 1 â€” Ingest a video

Click on /ingest

Click â€œTry it outâ€

Paste any YouTube link (with English subtitles)

Click Execute

Wait a few seconds â³
Youâ€™ll get something like:

{
  "video_url": "...",
  "status": "ingested",
  "chunks": 63
}


A folder named vectorstore will appear automatically â€” it stores your embeddings.

ğŸ”¹ Step 2 â€” Ask a question

Click on /query

Click â€œTry it outâ€

In the question box, type something like:

What is this video about?


Click Execute

After a few seconds, youâ€™ll see a meaningful answer from the local LLM ğŸ¯

âœ… Done!

Youâ€™ve now successfully:

Loaded a video

Built its embeddings

Queried it using RAG and a local model (no internet needed!)

ğŸ§  Optional

If you want to stop the server:

Ctrl + C


If you want to change model:

Edit .env â†’ OLLAMA_MODEL=llama3 (or any other model)

Pull the model using ollama pull llama3

Restart the backend.

----------------------------------------------------------------------



ğŸ Future Scope
Add frontend chat interface

Multi-video ingestion

Video summarization endpoint

Cloud LLM integration for faster inference

ğŸª„ Example Workflow
1ï¸âƒ£ Run backend
2ï¸âƒ£ In /docs, call /ingest with a YouTube URL
3ï¸âƒ£ Once status = ingested, call /query with a question
4ï¸âƒ£ Get the AI-generated answer! ğŸ¯



---

## âœ… Next Step for You
1. Copy this full markdown text  
2. Open your repoâ€™s `README.md` in VS Code or Notepad  
3. Replace everything inside it with this content  
4. Save the file  
5. Push it to GitHub:
   ```bash
   git add README.md
   git commit -m "Updated README with setup guide and model switch instructions"
   git push origin main
