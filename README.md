# ğŸ¥ RagTube â€” YouTube Video Q&A Assistant

**RagTube** is a Retrieval-Augmented Generation (RAG) based system that lets you **ask questions about any YouTube video**.  
It fetches the video transcript, splits it into chunks, generates embeddings, stores them in FAISS, and uses a local LLM (via Ollama) to answer user queries.

---

## ğŸš€ Features

- Fetches YouTube transcripts automatically (manual or auto captions)
- Cleans and chunks transcripts into meaningful sections
- Generates embeddings locally using Ollama
- Stores vectors in FAISS (local vector database)
- Retrieves the most relevant chunks for a given question
- Answers questions using a local LLM model (offline!)

---

## ğŸ§  Tech Stack

| Component | Technology |
|------------|-------------|
| **Backend** | FastAPI (Python) |
| **LLM Runtime** | Ollama |
| **Vector Database** | FAISS |
| **Embeddings** | `nomic-embed-text` (via Ollama) |
| **Language Model** | Default: `phi3` |
| **Transcript Fetching** | yt-dlp |
| **Language** | Python 3.10+ |

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/kartikeyaswarup-m1/RagTube.git
cd RagTube/backend
2ï¸âƒ£ Create and Activate Virtual Environment
bash

python -m venv venv
venv\Scripts\activate
3ï¸âƒ£ Install Dependencies
bash

pip install -r requirements.txt
(If thereâ€™s no requirements.txt, you can install manually)
pip install fastapi uvicorn yt-dlp requests ollama faiss-cpu numpy python-dotenv

4ï¸âƒ£ Ensure Ollama is Running
Install Ollama from https://ollama.com/ and pull required models:

bash

ollama pull phi3
ollama pull nomic-embed-text
ğŸ”§ Configuration
All configuration values are stored in the .env file located in backend/.env.

Default:

bash

# ===== Backend Config =====
VECTORSTORE_DIR=./vectorstore

# ===== Ollama Config =====
OLLAMA_MODEL=phi3
EMBED_MODEL=nomic-embed-text
OLLAMA_HOST=http://127.0.0.1:11434

# ===== FastAPI Config =====
BACKEND_HOST=127.0.0.1
BACKEND_PORT=8000
ğŸ” Switching to a Different Model
If you want to use a different model (for example, llama3 or gemma):

Pull the model in Ollama:

ollama pull llama3
Update this line in your .env file:

bash

OLLAMA_MODEL=llama3
Thatâ€™s it!
The backend automatically reads the model name from the .env file â€”
you do not need to modify any Python code.

â–¶ï¸ Running the Backend
Run the FastAPI server from the project root:

bash

cd C:\Users\HP\RagTube
uvicorn backend.app.main:app --reload
Youâ€™ll see:

arduino

INFO:     Uvicorn running on http://127.0.0.1:8000
Then open:
ğŸ‘‰ http://127.0.0.1:8000/docs

ğŸ§© API Endpoints
/ingest
Fetches transcript, chunks it, and builds FAISS vectorstore.

Query:

bash

GET /ingest?video_url=<YOUTUBE_URL>
Example:

bash

http://127.0.0.1:8000/ingest?video_url=https://youtu.be/RRVYpIET_RU
/query
Asks a question based on the ingested video.

Query:

bash

GET /query?question=<YOUR_QUESTION>
Example:

bash

http://127.0.0.1:8000/query?question=What is this video about?
ğŸ“¦ Output Files
After ingestion, youâ€™ll see:

pgsql

backend/vectorstore/
 â”œâ”€â”€ faiss.index
 â””â”€â”€ mapping.pkl
These files store embeddings for the ingested video transcript.



ğŸ Future Scope
Add frontend chat interface

Multi-video ingestion

Video summarization endpoint

Cloud LLM integration for faster inference

ğŸª„ Example Workflow
1ï¸âƒ£ Run backend
2ï¸âƒ£ In /docs, call /ingest with a YouTube URL
3ï¸âƒ£ Once status = ingested, call /query with a question
4ï¸âƒ£ Get the AI-generated answer! ğŸ¯



âœ¨ Built with â¤ï¸ using FastAPI, Ollama, and FAISS.

---

## âœ… Next Step for You
1. Copy this full markdown text  
2. Open your repoâ€™s `README.md` in VS Code or Notepad  
3. Replace everything inside it with this content  
4. Save the file  
5. Push it to GitHub:
   ```bash
   git add README.md
   git commit -m "Updated README with setup guide and model switch instructions"
   git push origin main
